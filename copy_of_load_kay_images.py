# -*- coding: utf-8 -*-
"""Copy of load_kay_images.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M4Sp_X-sQS4KQPmMikUDaJclPdn1_Ax1
"""

import os
import numpy as np
import matplotlib.pyplot as plt

fname = "kay_labels.npy"
if not os.path.exists(fname):
  !wget -qO $fname https://osf.io/r638s/download
fname = "kay_images.npz"
if not os.path.exists(fname):
  !wget -qO $fname https://osf.io/ymnjv/download

with np.load(fname) as dobj:
    dat = dict(**dobj)
labels = np.load('kay_labels.npy')

"""
`labels` is a 4 by stim array of class names:  
- row 3 has the labels predicted by a deep neural network (DNN) trained on Imagenet
- rows 0-2 correspond to different levels of the wordnet hierarchy for the DNN predictions"""

print(labels[:,:4])

print(dat.keys())

"""`dat` has the following fields:  
- `stimuli`: stim x i x j array of grayscale stimulus images
- `stimuli_test`: stim x i x j array of grayscale stimulus images in the test set  
- `responses`: stim x voxel array of z-scored BOLD response amplitude
- `responses_test`:  stim x voxel array of z-scored BOLD response amplitude in the test set  
- `roi`: array of voxel labels
- `roi_names`: array of names corresponding to voxel labels
"""

print(dat["stimuli"].shape)

print(dat["responses"].shape)

"""This is the number of voxels in each ROI. Note that `"Other"` voxels have been removed from this version of the dataset:"""

dict(zip(dat["roi_names"], np.bincount(dat["roi"])))

roi_name = dat["roi_names"]
roi = dat["roi"]

v1 = [idx for idx, value in enumerate(roi) if roi_name[value] == 'V1']
v2 = [idx for idx, value in enumerate(roi) if roi_name[value] == 'V2']
v3 = [idx for idx, value in enumerate(roi) if roi_name[value] == 'V3']
v3a = [idx for idx, value in enumerate(roi) if roi_name[value] == 'V3A']
v3b = [idx for idx, value in enumerate(roi) if roi_name[value] == 'V3B']
v4 = [idx for idx, value in enumerate(roi) if roi_name[value] == 'V4']
lat_occ = [idx for idx, value in enumerate(roi) if roi_name[value] == 'LatOcc']

import pandas as pd
import csv

x = dat["responses"]
x_test = dat["responses_test"]
y = dat["stimuli"]
y_test = dat["stimuli_test"]


labels_df = pd.read_csv('Kay_Gallant_labels_sorted.csv', header=None, 
                        names=['ID', 'Level0', 'Level1', 'Level2', 'Level3'],
                        skiprows=[0])

label_responses = {}
labels_ = {}
count = 1 
for idx in labels_df['ID']:
  stim_response = x[idx]
  label_name = labels_df.loc[labels_df['ID']==idx, ['Level0','Level1','Level2']].values
  label_responses[idx] = [stim_response]
  labels_[idx] = label_name 
  count = count+1

df_lab1 = pd.DataFrame.from_dict(labels_, orient = 'index').reset_index()
split_labels = [levels for items in df_lab1[0] for levels in items]
df_lab2 = pd.DataFrame(split_labels)
df_labels = pd.concat([df_lab1['index'],df_lab2] , axis =1)

df_res1 = pd.DataFrame.from_dict(label_responses, orient='index').reset_index()
df_res2 = pd.DataFrame(df_res1[0].to_list())

df_v1 = df_res2[v1]
df_v2 = df_res2[v2]
df_v3 = df_res2[v3]
df_v3a = df_res2[v3a]
df_v3b = df_res2[v3b]
df_v4 = df_res2[v4]
df_LO = df_res2[lat_occ]

df_stimuli = pd.concat([df_res1['index'],df_v1,df_v2,df_v3,
                        df_v3a,df_v3b,df_v4,df_LO], axis = 1)


data_df = df_labels.merge(df_stimuli, on='index').drop('index',axis=1)
less_5 = data_df['0_x'].isin(['plant','person','fungus'])
data_df = data_df[~less_5]
print(data_df)

test_df = pd.DataFrame.from_dict(x_test)
print(test_df)

"""
Each stimulus is associated with a pattern of BOLD response across voxels in visual cortex:"""

###### Level 0 #############
grouped_data = data_df.groupby('0_x')
groups = {}
for name, group in grouped_data:
  print(name)
  groups[name]=group.iloc[0:150,0:8430]
final_df = pd.concat(groups)
final_df

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt 
# %matplotlib inline
import seaborn as sns
from sklearn.utils import shuffle

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import warnings
from sklearn.exceptions import ConvergenceWarning, UndefinedMetricWarning

with warnings.catch_warnings():
    warnings.filterwarnings("ignore", category=(ConvergenceWarning))
    warnings.filterwarnings("ignore", category=(UndefinedMetricWarning))

def svm_model(features, labels, test_set):

  X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.20, random_state=42)

  scaler = StandardScaler()
  X_train_scaled = scaler.fit_transform(X_train)
  X_test_scaled = scaler.transform(X_test)
  test_set = scaler.transform(test_set)

  svm_model = OneVsRestClassifier(SVC(C=1.0,kernel='linear', max_iter=1000))
  svm_model.fit(X_train_scaled, y_train)

  Y_pred = svm_model.predict(X_test_scaled)
  test_predict = svn.model(test_set)

  print(f"Test Set Accuracy : {accuracy_score(y_test, Y_pred) * 100} %") 
  print("Training set score for SVM: %f" % svm_model.score(X_train_scaled , y_train))
  print("Testing  set score for SVM: %f" % svm_model.score(X_test_scaled  , y_test ))
  print('\n\n')
  classification_dict = classification_report(y_test,Y_pred, output_dict=True)  
  print("result ",test_predict)
  return Y_pred, classification_dict

####### LEVEL 0 ##########
V1features = final_df.iloc[:,3:1297]
V2features = final_df.iloc[:,1297:3380]
V3features = final_df.iloc[:,3380:5170]
V3Afeatures = final_df.iloc[:,5170:5654]
V3Bfeatures = final_df.iloc[:,5654:5968]
V4features = final_df.iloc[:,5968:7503]
LOfeatures = final_df.iloc[:,7503:8431]

test_V1 = test_df[:, 0:1294]
labels_0 = final_df.iloc[:,0]

V1_pred0, V1_rep0 = svm_model(V1features, labels_0, test_v1)
V2_pred0, V2_rep0 = svm_model(V2features, labels_0)
V3_pred0, V3_rep0 = svm_model(V3features, labels_0)
V3A_pred0, V3A_rep0 = svm_model(V3Afeatures, labels_0)
V3B_pred0, V3B_rep0 = svm_model(V3Bfeatures, labels_0)
V4_pred0, V4_rep0 = svm_model(V4features, labels_0)
LO_pred0, LO_rep0 = svm_model(LOfeatures, labels_0)

####### LEVEL 1 ##########

labels_1 = final_df.iloc[:,1]

V1_pred1, V1_rep1 = svm_model(V1features, labels_1)
V2_pred1, V2_rep1 = svm_model(V2features, labels_1)
V3_pred1, V3_rep1 = svm_model(V3features, labels_1)
V3A_pred1, V3A_rep1 = svm_model(V3Afeatures, labels_1)
V3B_pred1, V3B_rep1 = svm_model(V3Bfeatures, labels_1)
V4_pred1, V4_rep1 = svm_model(V4features, labels_1)
LO_pred1, LO_rep1 = svm_model(LOfeatures, labels_1)

labels_2 = final_df.iloc[:,2]

V1_pred2, V1_rep2 = svm_model(V1features, labels_2)
V2_pred2, V2_rep2 = svm_model(V2features, labels_2)
V3_pred2, V3_rep2 = svm_model(V3features, labels_2)
V3A_pred2, V3A_rep2 = svm_model(V3Afeatures, labels_2)
V3B_pred2, V3B_rep2 = svm_model(V3Bfeatures, labels_2)
V4_pred2, V4_rep2 = svm_model(V4features, labels_2)
LO_pred2, LO_rep2 = svm_model(LOfeatures, labels_2)

def prepare_df(prediction_list):
  
  df_list = []
  for prediction in prediction_list:
    df = pd.DataFrame(prediction)
    df_list.append(df)
  prediction_df = pd.concat(df_list)
  prediction_df['Data'] = prediction_df.index
  precision = prediction_df[prediction_df['Data']=='precision']
  precision['ROI'] = ['V1','V2','V3','V3A','V3B','V4','Lat Occ']

  return precision

predictions_0 = [V1_rep0,V2_rep0,V3_rep0,V3A_rep0,V3B_rep0,V4_rep0,LO_rep0]
precision0 = prepare_df(predictions_0)

predictions_1 = [V1_rep1,V2_rep1,V3_rep1,V3A_rep1,V3B_rep1,V4_rep1,LO_rep1]
precision1 = prepare_df(predictions_1)

predictions_2 = [V1_rep2,V2_rep2,V3_rep2,V3A_rep2,V3B_rep2,V4_rep2,LO_rep2]
precision2 = prepare_df(predictions_2)

precision0['Level0'] = precision0['accuracy']
precision0['Level1'] = precision1['accuracy']
precision0['Level2'] = precision2['accuracy']

result_df = precision0[['Level0','Level1','Level2','ROI']]
result_df['Level0'] = result_df['Level0'].apply(lambda x: "{0:.2f}".format(x*100))
result_df['Level1'] = result_df['Level1'].apply(lambda x: "{0:.2f}".format(x*100))
result_df['Level2'] = result_df['Level2'].apply(lambda x: "{0:.2f}".format(x*100))

sns.barplot(x='ROI', y='Level0', data=result_df)
sns.barplot(x='ROI', y='Level1', data=result_df)
sns.barplot(x='ROI', y='Level2', data=result_df)
plt.show()

f, ax = plt.subplots(figsize=(12, 5))
ax.set(xlabel="Voxel", ylabel= "animal")
heatmap = ax.imshow(groups["animal"], aspect="auto", vmin=-1, vmax=1, cmap="bwr")
f.colorbar(heatmap, shrink=.5, label="Response amplitude (Z)")
f.tight_layout()

f, ax = plt.subplots(figsize=(12, 5))
ax.set(xlabel="Voxel", ylabel="artifact")
heatmap = ax.imshow(groups["artifact"], aspect="auto", vmin=-1, vmax=1, cmap="bwr")
f.colorbar(heatmap, shrink=.5, label="Response amplitude (Z)")
f.tight_layout()

f, ax = plt.subplots(figsize=(12, 5))
ax.set(xlabel="Voxel", ylabel="entity")
heatmap = ax.imshow(groups["entity"], aspect="auto", vmin=-1, vmax=1, cmap="bwr")
f.colorbar(heatmap, shrink=.5, label="Response amplitude (Z)")
f.tight_layout()

f, ax = plt.subplots(figsize=(12, 5))
ax.set(xlabel="Voxel", ylabel="fruit")
heatmap = ax.imshow(groups["fruit"], aspect="auto", vmin=-1, vmax=1, cmap="bwr")
f.colorbar(heatmap, shrink=.5, label="Response amplitude (Z)")
f.tight_layout()

f, ax = plt.subplots(figsize=(12, 5))
ax.set(xlabel="Voxel", ylabel="fungus")
heatmap = ax.imshow(groups["fungus"], aspect="auto", vmin=-1, vmax=1, cmap="bwr")
f.colorbar(heatmap, shrink=.5, label="Response amplitude (Z)")
f.tight_layout()

f, ax = plt.subplots(figsize=(12, 5))
ax.set(xlabel="Voxel", ylabel="geological formation")
heatmap = ax.imshow(groups["geological formation"], aspect="auto", vmin=-1, vmax=1, cmap="bwr")
f.colorbar(heatmap, shrink=.5, label="Response amplitude (Z)")
f.tight_layout()

f, ax = plt.subplots(figsize=(12, 5))
ax.set(xlabel="Voxel", ylabel="person")
heatmap = ax.imshow(groups["person"], aspect="auto", vmin=-1, vmax=1, cmap="bwr")
f.colorbar(heatmap, shrink=.5, label="Response amplitude (Z)")
f.tight_layout()

f, ax = plt.subplots(figsize=(12, 5))
ax.set(xlabel="Voxel", ylabel="plant")
heatmap = ax.imshow(groups["plant"], aspect="auto", vmin=-1, vmax=1, cmap="bwr")
f.colorbar(heatmap, shrink=.5, label="Response amplitude (Z)")
f.tight_layout()

"""The training/validation splits from the original paper are retained, so the 120 test stimuli and responses are in separate data structures:"""

f, ax = plt.subplots(figsize=(12, 2.5))
ax.set(xlabel="Voxel", ylabel="Test Stimulus")
heatmap = ax.imshow(dat["responses_test"], aspect="auto", vmin=-1, vmax=1, cmap="bwr")
f.colorbar(heatmap, shrink=.75, label="Response amplitude (Z)")
f.tight_layout()